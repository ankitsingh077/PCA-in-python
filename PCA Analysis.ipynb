{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the general demographics data.\n",
    "azdias = pd.read_csv('Udacity_AZDIAS_Subset.csv', sep = ';')\n",
    "\n",
    "# Load in the feature summary file.\n",
    "feat_info = pd.read_csv('AZDIAS_Feature_Summary.csv', sep = ';')\n",
    "print(feat_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn missing_or_unknown to list \n",
    "feat_info['missing_or_unknown'] = feat_info['missing_or_unknown'].apply(lambda x: x[1:-1].split(','))\n",
    "\n",
    "# Identify missing or unknown data values and convert them to NaNs.\n",
    "for attrib, missing_values in zip(feat_info['attribute'], feat_info['missing_or_unknown']):\n",
    "    if missing_values[0] != '':\n",
    "        for value in missing_values:\n",
    "            if value.isnumeric() or value.lstrip('-').isnumeric():\n",
    "                value = int(value)\n",
    "            azdias.loc[azdias[attrib] == value, attrib] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an assessment of how much missing data there is in each column of the\n",
    "# dataset.\n",
    "missing_data = pd.Series(azdias.isnull().sum() / len(azdias))\n",
    "missing_data.plot(kind='barh', figsize=(10, 20))\n",
    "missing_data[missing_data > 0.2].index.tolist()\n",
    "['AGER_TYP',\n",
    " 'GEBURTSJAHR',\n",
    " 'TITEL_KZ',\n",
    " 'ALTER_HH',\n",
    " 'KK_KUNDENTYP',\n",
    " 'KBA05_BAUMAX']\n",
    "# Remove the outlier columns from the dataset. (You'll perform other data engineering tasks such as re-encoding and imputation later.)\n",
    "azdias = azdias.drop(['AGER_TYP','GEBURTSJAHR','TITEL_KZ','ALTER_HH','KK_KUNDENTYP','KBA05_BAUMAX'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data is missing in each row of the dataset?\n",
    "missing_data_rows = azdias.isnull().sum(axis = 1)\n",
    "missing_data_rows_low = azdias[azdias.isnull().sum(axis=1) < 10].reset_index(drop=True)\n",
    "\n",
    "missing_data_rows_high = azdias[azdias.isnull().sum(axis = 1) >= 10].reset_index(drop=True)\n",
    "def countplot(columns, num):\n",
    "    fig, axs = plt.subplots(num, 2, figsize=(15, 15))\n",
    "    fig.subplots_adjust(hspace =2 , wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for i in range(num):\n",
    "        \n",
    "        sns.countplot(missing_data_rows_low[columns[i]], ax=axs[i*2])\n",
    "        axs[i*2].set_title('missing_data_rows_low')\n",
    "        sns.countplot(missing_data_rows_high[columns[i]], ax=axs[i*2+1])\n",
    "        axs[i*2+1].set_title('missing_data_rows_high')\n",
    "        \n",
    "countplot(missing_data_rows_high.columns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Perform feature trimming, re-encoding, and engineering for demographics\n",
    "    data\n",
    "    \n",
    "    INPUT: Demographics DataFrame\n",
    "    OUTPUT: Trimmed and cleaned demographics DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put in code here to execute all main cleaning steps:\n",
    "    # convert missing value codes into NaNs, ...\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "\n",
    "# Identify missing or unknown data values and convert them to NaNs.\n",
    "    for col_name in df.columns:\n",
    "        df_copy[col_name] = df_copy[col_name].map(lambda x: np.nan if str(x) in feat_info.loc[col_name].missing_or_unknown else x)\n",
    "        \n",
    "    # remove selected columns and rows, ...\n",
    "    c_removed =['AGER_TYP','GEBURTSJAHR','TITEL_KZ','ALTER_HH','KK_KUNDENTYP','KBA05_BAUMAX']\n",
    "    \n",
    "    for c in c_removed:\n",
    "        df_copy.drop(c, axis=1, inplace=True)\n",
    "        \n",
    "    df_copy = df_copy[df_copy.isnull().sum(axis=1) < 10].reset_index(drop=True)\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        df_copy[col] = df_copy[col].fillna(df_copy[col].mode()[0])\n",
    "        \n",
    "    \n",
    "    # select, re-encode, and engineer column values.\n",
    "    multi_level = []\n",
    "    for column in df_copy.columns:\n",
    "        if feat_info.loc[column].type == 'categorical' and len(df_copy[column].unique()) > 2:\n",
    "            multi_level.append(column)\n",
    "            \n",
    "    for col in multi_level:\n",
    "        df_copy.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    df_copy['decade'] = df_copy['PRAEGENDE_JUGENDJAHRE'].apply(create_interval_decade)\n",
    "    df_copy['movement'] = df_copy['PRAEGENDE_JUGENDJAHRE'].apply(create_binary_movement)\n",
    "    df_copy.drop('PRAEGENDE_JUGENDJAHRE', axis=1, inplace=True)\n",
    "    \n",
    "    df_copy['wealth'] = df_copy['CAMEO_INTL_2015'].apply(wealth)\n",
    "    df_copy['life_stage'] = df_copy['CAMEO_INTL_2015'].apply(life_stage)\n",
    "    df_copy.drop('CAMEO_INTL_2015', axis=1, inplace=True)\n",
    "    \n",
    "    df_copy = pd.get_dummies(data=df_copy, columns=['OST_WEST_KZ'])\n",
    "    \n",
    "    mixed = ['LP_LEBENSPHASE_FEIN','LP_LEBENSPHASE_GROB','WOHNLAGE','PLZ8_BAUMAX']\n",
    "    \n",
    "    for c in mixed:\n",
    "        df_copy.drop(c, axis=1, inplace=True)\n",
    "    \n",
    "     # Return the cleaned dataframe.\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the Nan values with the mode of that respective column.\n",
    "for col in missing_data_rows_low.columns:\n",
    "        missing_data_rows_low[col] = missing_data_rows_low[col].fillna(missing_data_rows_low[col].mode()[0])\n",
    "# Apply feature scaling to the general population demographics data.\n",
    "normalizer = StandardScaler()\n",
    "missing_data_rows_low[missing_data_rows_low.columns] = normalizer.fit_transform(missing_data_rows_low[missing_data_rows_low.columns])\n",
    "missing_data_rows_low.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the data.\n",
    "\n",
    "pca = PCA()\n",
    "missing_data_rows_low_pca = pca.fit_transform(missing_data_rows_low)\n",
    "# Investigate the variance accounted for by each principal component.\n",
    "def scree_plot(pca):\n",
    "    '''\n",
    "    Creates a scree plot associated with the principal components \n",
    "    \n",
    "    INPUT: pca - the result of instantian of PCA in scikit learn\n",
    "            \n",
    "    OUTPUT:\n",
    "            None\n",
    "    '''\n",
    "    num_components = len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=12)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')\n",
    "    \n",
    "\n",
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-apply PCA to the data while selecting for number of components to retain.\n",
    "\n",
    "pca = PCA(n_components=41)\n",
    "missing_data_rows_low_pca = pca.fit_transform(missing_data_rows_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_score(data, center):\n",
    "    '''\n",
    "    returns the kmeans score regarding SSE for points to centers\n",
    "    INPUT:\n",
    "        data - the dataset you want to fit kmeans to\n",
    "        center - the number of centers you want (the k value)\n",
    "    OUTPUT:\n",
    "        score - the SSE score for the kmeans model fit to the data\n",
    "    '''\n",
    "    #instantiate kmeans\n",
    "    kmeans = KMeans(n_clusters=center)\n",
    "\n",
    "    # Then fit the model to your data using the fit method\n",
    "    model = kmeans.fit(data)\n",
    "    \n",
    "    # Obtain a score related to the model fit\n",
    "    score = np.abs(model.score(data))\n",
    "    \n",
    "    return score\n",
    "# Over a number of different cluster counts...\n",
    "# run k-means clustering on the data and...\n",
    "# compute the average within-cluster distances.\n",
    "scores = []\n",
    "centers = list(range(1,30,3))\n",
    "\n",
    "for center in centers:\n",
    "    scores.append(get_kmeans_score(missing_data_rows_low_pca, center))\n",
    "# Investigate the change in within-cluster distance across number of clusters.\n",
    "# HINT: Use matplotlib's plot function to visualize this relationship.\n",
    "plt.plot(centers, scores, linestyle='--', marker='o', color='b');\n",
    "plt.xlabel('K');\n",
    "plt.ylabel('SSE');\n",
    "plt.title('SSE vs. K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fit the k-means model with the selected number of clusters and obtain\n",
    "# cluster predictions for the general population demographics data.\n",
    "\n",
    "# Re-fit the k-means model with the selected number of clusters and obtain\n",
    "# cluster predictions for the general population demographics data.\n",
    "\n",
    "kmeans = KMeans(n_clusters=22)\n",
    "model_general = kmeans.fit(missing_data_rows_low_pca)\n",
    "predict_general = model_general.predict(missing_data_rows_low_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of people are part of a cluster that is overrepresented in the\n",
    "# customer data compared to the general population?\n",
    "over = normalizer.inverse_transform(pca.inverse_transform(customers_clean_pca[np.where(predict_customers==11)])).round()\n",
    "df_over = pd.DataFrame(data = over, columns = customers_clean.columns)\n",
    "df_over.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of people are part of a cluster that is underrepresented in the\n",
    "# customer data compared to the general population?\n",
    "under = normalizer.inverse_transform(pca.inverse_transform(customers_clean_pca[np.where(predict_customers==16)])).round()\n",
    "df_under = pd.DataFrame(data=under, columns=customers_clean.columns)\n",
    "df_under.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
